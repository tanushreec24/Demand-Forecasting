{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4fddab5",
   "metadata": {},
   "source": [
    "# e-commerce web scraping test notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88531bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_style('dark')\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from scrapy.http import Request\n",
    "\n",
    "import re\n",
    "\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93f6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from scrapy import Request, Spider, Selector\n",
    "import scrapy\n",
    "from ecommerce_scraper.ecommerce_scraper.items import EcommerceScraperItem\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b8a7cc6",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da8f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = 'https://XXXXX.co.th/api/v4/pages/get_category_tree'\n",
    "content = requests.get(main_url)\n",
    "main_dict = json.loads(content.text)\n",
    "# list of categoreis\n",
    "categories = main_dict['data']['category_list']\n",
    "# regular shop only (no mall)\n",
    "start_urls = []\n",
    "for category in categories:\n",
    "    catid = category['catid']\n",
    "    name = category['name'].replace(\" \", \"-\")\n",
    "    children_id = [children['catid'] for children in category['children']]\n",
    "    # append root page\n",
    "    url = f\"https://XXXXX.co.th/{name}-cat.{catid}\"\n",
    "    start_urls.append(url)\n",
    "    # append children pages\n",
    "    for id in children_id:\n",
    "        for i in range(9):\n",
    "            url = f\"https://XXXXX.co.th/{name}-cat.{catid}.{id}?page={i}\"\n",
    "            start_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a8c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_urls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "648002cb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53cdcaca",
   "metadata": {},
   "source": [
    "avoid login by quit, wait and retry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e6024bb",
   "metadata": {},
   "source": [
    "### Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be031c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from scrapy import Selector\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "916952d7",
   "metadata": {},
   "source": [
    "### Define additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32707fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    proxies_source = Selector(text=response.text)\n",
    "    proxies = set()\n",
    "    for element in proxies_source.xpath('//tbody/tr'):\n",
    "        if element.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            #Grabbing IP and corresponding PORT\n",
    "            proxy = \":\".join([element.xpath('.//td[1]/text()').get(), element.xpath('.//td[2]/text()').get()])\n",
    "            proxies.add(proxy)\n",
    "    return list(proxies)\n",
    "\n",
    "def get_default_option(headless=False):\n",
    "    chrome_options = Options()\n",
    "    # selenium non-headless/headless option\n",
    "    #options.headless = True\n",
    "    chrome_options.headless = headless\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    chrome_options.add_argument(\"--user-agent={}\".format(user_agent))\n",
    "    return chrome_options\n",
    "\n",
    "def start_new_selenium_session(url_new_session, headless=False, custom_proxies_list=[]):\n",
    "    \n",
    "    # if custom proxies are provided\n",
    "    if len(custom_proxies_list) != 0:\n",
    "        for i in range(len(custom_proxies_list)):\n",
    "            chrome_options = get_default_option(headless)\n",
    "            # get a proxy from the pool\n",
    "            print(\"Proxy #%d\"%i)\n",
    "            custom_proxy = custom_proxies_list[i]\n",
    "            chrome_options.add_argument(\"--proxy-server={}\".format(custom_proxy))\n",
    "            try:\n",
    "                driver = webdriver.Chrome(options=chrome_options)\n",
    "                driver.set_window_size(1920, 1080)\n",
    "                driver.maximize_window()\n",
    "                driver.get(url_new_session)\n",
    "\n",
    "                wait = WebDriverWait(driver, 4)\n",
    "\n",
    "                select_language_xpath = '//*[@class=\"language-selection__list\"]/div[2]'\n",
    "                wait.until(EC.element_to_be_clickable((By.XPATH, select_language_xpath)))\n",
    "\n",
    "                actions = ActionChains(driver)\n",
    "                actions.click(driver.find_element(By.XPATH, select_language_xpath))\n",
    "                actions.perform()\n",
    "\n",
    "                return driver\n",
    "            except:\n",
    "                print(\"Skipping. Connnection error\")\n",
    "                driver.quit()\n",
    "    \n",
    "    # no custom proxies are provided or all proxies in custom_proxies_list are used\n",
    "    chrome_options = get_default_option(headless)\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.set_window_size(1920, 1080)\n",
    "    driver.maximize_window()\n",
    "    driver.get(url_new_session)\n",
    "\n",
    "    wait = WebDriverWait(driver, 4)\n",
    "\n",
    "    select_language_xpath = '//*[@class=\"language-selection__list\"]/div[2]'\n",
    "    wait.until(EC.element_to_be_clickable((By.XPATH, select_language_xpath)))\n",
    "\n",
    "    actions = ActionChains(driver)\n",
    "    actions.click(driver.find_element(By.XPATH, select_language_xpath))\n",
    "    actions.perform()\n",
    "\n",
    "    return driver\n",
    "\n",
    "def write_to_json(lst: list, fn: str):\n",
    "    # with open(fn, 'a', encoding='utf-8') as file:\n",
    "    with open(fn, 'w', encoding='utf-8') as file:\n",
    "        for item in lst:\n",
    "            x = json.dumps(item, ensure_ascii=False ,indent=4)\n",
    "            file.write(x + '\\n')\n",
    "#export to JSON\n",
    "# write_to_json(data, 'elements.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38821549",
   "metadata": {},
   "source": [
    "### Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c533f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET URLs\n",
    "main_url = 'https://XXXXX.co.th/api/v4/pages/get_category_tree'\n",
    "content = requests.get(main_url)\n",
    "main_dict = json.loads(content.text)\n",
    "# list of categoreis\n",
    "categories = main_dict['data']['category_list']\n",
    "# regular shop only (no mall)\n",
    "start_urls = []\n",
    "for category in categories:\n",
    "    catid = category['catid']\n",
    "    name = category['name'].replace(\" \", \"-\")\n",
    "    children_id = [children['catid'] for children in category['children']]\n",
    "    # append root page\n",
    "    url = f\"https://XXXXX.co.th/{name}-cat.{catid}\"\n",
    "    start_urls.append(url)\n",
    "    # append children pages\n",
    "    for id in children_id:\n",
    "        for i in range(5):\n",
    "            url = f\"https://XXXXX.co.th/{name}-cat.{catid}.{id}?page={i}\"\n",
    "            start_urls.append(url)\n",
    "\n",
    "proxies_list = get_proxies()\n",
    "driver = start_new_selenium_session(url_new_session='https://XXXXX.co.th/Men-Clothes-cat.11044945', headless=False)\n",
    "\n",
    "# create string of feature that we needed to extract from webpage\n",
    "features = [\n",
    "    'shop_id',\n",
    "    'product_id',\n",
    "    'product_name',\n",
    "    'cat_1',\n",
    "    'cat_2',\n",
    "    'cat_3',\n",
    "    'price',\n",
    "    'discount',\n",
    "    'star',\n",
    "    'rating',\n",
    "    'sold',\n",
    "    'color_available',\n",
    "    'size_available',\n",
    "    'stock',\n",
    "    'favorite',\n",
    "    'product_reviews',\n",
    "    'shop_verified',\n",
    "    'shop_preferred_plus_seller',\n",
    "    'shop_location',\n",
    "    'shop_product',\n",
    "    'shop_response_rate',\n",
    "    'shop_response_time',\n",
    "    'shop_joined',\n",
    "    'shop_followers',\n",
    "    'shop_star',\n",
    "    'shop_rating_bad',\n",
    "    'shop_rating_good',\n",
    "    'shop_rating_normal'\n",
    "]\n",
    "\n",
    "data = list()\n",
    "product_number = 1\n",
    "\n",
    "for url in start_urls:\n",
    "    driver.get(url)\n",
    "    wait = WebDriverWait(driver, 4)\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, '//*[@class=\"row XXXXX-search-item-result__items\"]/div[last()]')))\n",
    "    \n",
    "    # slowly scroll down to the end of the page\n",
    "    total_height = int(driver.execute_script(\"return document.body.scrollHeight\"))\n",
    "    for i in range(1, total_height, 100):\n",
    "        driver.execute_script(\"window.scrollTo(0, {});\".format(i))\n",
    "\n",
    "    sel_source = Selector(text=driver.page_source)\n",
    "\n",
    "    products = sel_source.xpath(\"//div[contains(@class, 'col-xs-2-4 XXXXX-search-item-result__item')]/a/@href\").getall()\n",
    "    for product_url in products:\n",
    "        url_sub = urljoin(main_url, product_url)\n",
    "\n",
    "        driver.execute_script(\"window.open('');\")\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        driver.get(url_sub)\n",
    "\n",
    "        # wait until elements loaded\n",
    "        # wait.until(EC.presence_of_element_located((By.XPATH, '//button[@class=\"btn btn-solid-primary btn--l vQ3lCI\"]')))\n",
    "        # at this point, login page would occur if bot detected\n",
    "        retry = True\n",
    "        while retry == True:\n",
    "            try:\n",
    "                wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"main\"]/div/div[2]/div[1]/div/div/div/div[2]/div[3]/div/div[5]/div/div/button[2]')))\n",
    "                retry = False\n",
    "            except:\n",
    "                driver.quit()\n",
    "                print(\"Bot Detected... retrying\")\n",
    "                # time.sleep(61)\n",
    "                proxies_list = get_proxies()\n",
    "                driver = start_new_selenium_session(url, headless=False)\n",
    "                driver.execute_script(\"window.open('');\")\n",
    "                driver.switch_to.window(driver.window_handles[1])\n",
    "                driver.get(url_sub)\n",
    "                wait = WebDriverWait(driver, 10)\n",
    "                \n",
    "        # scroll page\n",
    "        while True :\n",
    "            for i in range(1, total_height, 100):\n",
    "                driver.execute_script(\"window.scrollTo(0, {});\".format(i))\n",
    "            if EC.presence_of_element_located((By.XPATH, '//div[@class=\"_4Zf0hW PHkozQ\"]')) == True:\n",
    "                break\n",
    "            break\n",
    "\n",
    "        sel_source = Selector(text=driver.page_source)\n",
    "\n",
    "        #define pattern for regular expression\n",
    "        pattern = re.compile('(?:-i).(\\d*).(\\d*)')\n",
    "\n",
    "        # extract the data from the response using CSS or XPath selectors\n",
    "        shop_id = pattern.search(url_sub).group(1)\n",
    "        product_id = pattern.search(url_sub).group(2)\n",
    "        product_name = sel_source.xpath('//div[@class=\"YPqix5\"]/span/text()').get()\n",
    "        cat = sel_source.xpath('//a[@class=\"_4Zf0hW PHkozQ\"]/text()').getall()\n",
    "        cat_1 = cat[1]\n",
    "        try:\n",
    "            cat_2 = cat[2]\n",
    "        except:\n",
    "            cat_2 = \"\"\n",
    "        try:\n",
    "            cat_3 = cat[3]\n",
    "        except:\n",
    "            cat_3 = \"\"\n",
    "\n",
    "        price_detail_element = sel_source.xpath('//div[@class=\"flex items-center\"]/div/text()').getall()\n",
    "        price = price_detail_element[0]\n",
    "        if len(price_detail_element) != 1:\n",
    "            discount = price_detail_element[1]\n",
    "        star =  sel_source.xpath('//div[@class=\"yz-vZm _2qXJwX\"]/text()').get()\n",
    "        rating = sel_source.xpath('//div[@class=\"yz-vZm\"]/text()').get()\n",
    "        sold = sel_source.xpath('//div[@class=\"yiMptB\"]/text()').get()\n",
    "        color_available = sel_source.xpath('//div[@class=\"flex flex-column\"]/div[1]/div/button/text()').getall()\n",
    "        size_available = sel_source.xpath('//div[@class=\"flex flex-column\"]/div[2]/div/button/text()').getall()\n",
    "        stock = sel_source.xpath('//div[@class=\"flex flex-column\"]/div[last()]/div[2]/div[2]/text()').get().split()[0]\n",
    "        favorite = sel_source.xpath('//div[@class=\"_7na4jG\"]/text()').getall()[1].split()[1][1:-1]\n",
    "\n",
    "        # extract reviews: [number_of_review, comment, likes]\n",
    "        comments = sel_source.xpath('//div[@class=\"EXI9SU\"]/text()').getall()\n",
    "        likes = sel_source.xpath('//div[@class=\"XXXXX-product-rating__like-count\"]/text()').getall()\n",
    "        number_reviews = len(sel_source.xpath('//div[@class=\"XXXXX-product-comment-list\"]/div').getall())\n",
    "        product_reviews = [[i, comments[i], likes[i]] for i in range(number_reviews)]\n",
    "        \n",
    "        # shop info\n",
    "        shop_info_url = f'https://XXXXX.co.th/api/v4/product/get_shop_info?shopid={shop_id}'\n",
    "        info = requests.get(shop_info_url)\n",
    "        info_dict = json.loads(info.text)\n",
    "\n",
    "        shop_verified = info_dict['data'][\"is_XXXXX_verified\"]\n",
    "        shop_preferred_plus_seller = info_dict['data'][\"is_preferred_plus_seller\"]\n",
    "        shop_location = info_dict['data'][\"shop_location\"]\n",
    "        shop_product = info_dict['data'][\"item_count\"]\n",
    "        shop_response_rate = info_dict['data'][\"response_rate\"]\n",
    "        shop_response_time = info_dict['data'][\"response_time\"]\n",
    "        shop_last_active = info_dict['data'][\"last_active_time\"]\n",
    "        shop_joined = sel_source.xpath('//div[@class=\"s1qcwz\"]/div[3]/div[1]/span/text()').get().split()\n",
    "        shop_followers = info_dict['data'][\"follower_count\"]\n",
    "        shop_star = info_dict['data'][\"rating_star\"]\n",
    "        shop_rating_bad = info_dict['data'][\"rating_bad\"]\n",
    "        shop_rating_good = info_dict['data'][\"rating_good\"]\n",
    "        shop_rating_normal = info_dict['data'][\"rating_normal\"]\n",
    "\n",
    "        ## create an instance of the item\n",
    "        # item = EcommerceScraperItem()\n",
    "        item = dict()\n",
    "\n",
    "        # assign the extracted data to the item field\n",
    "        for feature in features:\n",
    "            exec(f\"item['{feature}'] = {feature}\")\n",
    "        \n",
    "        ## check each extracted product \n",
    "        # print(item)\n",
    "        data.append(item)\n",
    "        print(f\"extracted product: {product_number}\")\n",
    "        product_number += 1\n",
    "\n",
    "        ## write json after each product extracted\n",
    "        write_to_json([data], 'data.json')\n",
    "\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "driver.quit()\n",
    "\n",
    "print(\"scraping completed\")\n",
    "print(f\"{product_number} products are extracted\")\n",
    "\n",
    "# write json after all product extracted\n",
    "# write_to_json([data], 'data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a2628",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_json([data], 'data.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65331d9e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff08c266",
   "metadata": {},
   "source": [
    "LogIn Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be031c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from scrapy import Selector\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32707fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selenium_login(driver):\n",
    "    wait = WebDriverWait(driver, 4)\n",
    "\n",
    "    wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[@class='pDzPRp' and @type='text']\"))).send_keys('william_watchh@hotmail.com')\n",
    "    wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[@class='pDzPRp' and @type='password']\"))).send_keys('Noppawam1')\n",
    "    login_confirm_xpath = \"//*[@class='wyhvVD _1EApiB hq6WM5 L-VL8Q cepDQ1 _7w24N1']\"\n",
    "    wait.until(EC.element_to_be_clickable((By.XPATH, login_confirm_xpath)))\n",
    "\n",
    "    actions = ActionChains(driver)\n",
    "    actions.click(driver.find_element(By.XPATH, login_confirm_xpath))\n",
    "    actions.perform()\n",
    "    return\n",
    "\n",
    "def start_new_selenium_session(url_new_session, headless=False):\n",
    "    options = Options()\n",
    "    # selenium non-headless/headless option\n",
    "    #options.headless = True\n",
    "    options.headless = headless\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.set_window_size(1920, 1080)\n",
    "    driver.maximize_window()\n",
    "    driver.get(url_new_session)\n",
    "\n",
    "    wait = WebDriverWait(driver, 4)\n",
    "\n",
    "    select_language_xpath = '//*[@class=\"language-selection__list\"]/div[2]'\n",
    "    wait.until(EC.element_to_be_clickable((By.XPATH, select_language_xpath)))\n",
    "\n",
    "    actions = ActionChains(driver)\n",
    "    actions.click(driver.find_element(By.XPATH, select_language_xpath))\n",
    "    actions.perform()\n",
    "\n",
    "    login_button_xpath = '//*[@id=\"main\"]/div/div[2]/header/div[1]/nav/ul/a[3]'\n",
    "    wait.until(EC.element_to_be_clickable((By.XPATH, login_button_xpath)))\n",
    "    actions = ActionChains(driver)\n",
    "    actions.click(driver.find_element(By.XPATH, login_button_xpath))\n",
    "    actions.perform()\n",
    "\n",
    "    selenium_login(driver)\n",
    "\n",
    "    return driver\n",
    "\n",
    "def write_to_json(lst: list, fn: str):\n",
    "    # with open(fn, 'a', encoding='utf-8') as file:\n",
    "    with open(fn, 'w', encoding='utf-8') as file:\n",
    "        for item in lst:\n",
    "            x = json.dumps(item, ensure_ascii=False ,indent=4)\n",
    "            file.write(x + '\\n')\n",
    "#export to JSON\n",
    "# write_to_json(data, 'elements.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed33c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET URLs\n",
    "main_url = 'https://XXXXX.co.th/api/v4/pages/get_category_tree'\n",
    "content = requests.get(main_url)\n",
    "main_dict = json.loads(content.text)\n",
    "# list of categoreis\n",
    "categories = main_dict['data']['category_list']\n",
    "# regular shop only (no mall)\n",
    "start_urls = []\n",
    "for category in categories:\n",
    "    catid = category['catid']\n",
    "    name = category['name'].replace(\" \", \"-\")\n",
    "    children_id = [children['catid'] for children in category['children']]\n",
    "    # append root page\n",
    "    url = f\"https://XXXXX.co.th/{name}-cat.{catid}\"\n",
    "    start_urls.append(url)\n",
    "    # append children pages\n",
    "    for id in children_id:\n",
    "        for i in range(9):\n",
    "            url = f\"https://XXXXX.co.th/{name}-cat.{catid}.{id}?page={i}\"\n",
    "            start_urls.append(url)\n",
    "\n",
    "driver = start_new_selenium_session(url_new_session='https://XXXXX.co.th/Men-Clothes-cat.11044945')\n",
    "\n",
    "# create string of feature that we needed to extract from webpage\n",
    "features = [\n",
    "    'shop_id',\n",
    "    'product_id',\n",
    "    'product_name',\n",
    "    'cat_1',\n",
    "    'cat_2',\n",
    "    'cat_3',\n",
    "    'price',\n",
    "    'discount',\n",
    "    'star',\n",
    "    'rating',\n",
    "    'sold',\n",
    "    'color_available',\n",
    "    'size_available',\n",
    "    'stock',\n",
    "    'favorite',\n",
    "    'shop_verified',\n",
    "    'shop_preferred_plus_seller',\n",
    "    'shop_location',\n",
    "    'shop_product',\n",
    "    'shop_response_rate',\n",
    "    'shop_response_time',\n",
    "    'shop_joined',\n",
    "    'shop_followers',\n",
    "    'shop_star',\n",
    "    'shop_rating_bad',\n",
    "    'shop_rating_good',\n",
    "    'shop_rating_normal'\n",
    "]\n",
    "\n",
    "data = list()\n",
    "product_number = 1\n",
    "\n",
    "for url in start_urls:\n",
    "    driver.get(url)\n",
    "    wait = WebDriverWait(driver, 4)\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, '//*[@class=\"row XXXXX-search-item-result__items\"]/div[last()]')))\n",
    "    \n",
    "    # slowly scroll down to the end of the page\n",
    "    total_height = int(driver.execute_script(\"return document.body.scrollHeight\"))\n",
    "    for i in range(1, total_height, 100):\n",
    "        driver.execute_script(\"window.scrollTo(0, {});\".format(i))\n",
    "\n",
    "    sel_source = Selector(text=driver.page_source)\n",
    "\n",
    "    products = sel_source.xpath(\"//div[contains(@class, 'col-xs-2-4 XXXXX-search-item-result__item')]/a/@href\").getall()\n",
    "    for product_url in products:\n",
    "        url_sub = urljoin(main_url, product_url)\n",
    "\n",
    "        driver.execute_script(\"window.open('');\")\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        driver.get(url_sub)\n",
    "\n",
    "        # wait until elements loaded\n",
    "        # wait.until(EC.presence_of_element_located((By.XPATH, '//button[@class=\"btn btn-solid-primary btn--l vQ3lCI\"]')))\n",
    "        retry = True\n",
    "        while retry == True:\n",
    "            try:\n",
    "                wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"main\"]/div/div[2]/div[1]/div/div/div/div[2]/div[3]/div/div[5]/div/div/button[2]')))\n",
    "                retry = False\n",
    "            except:\n",
    "                try:\n",
    "                    if EC.presence_of_element_located((By.XPATH, \"//*[@class='pDzPRp' and @type='text']\")) == True:\n",
    "                        selenium_login(driver)\n",
    "\n",
    "                        back_button_xpath = \"//*[@class='wyhvVD _1EApiB c5aL1I s2GSaS cepDQ1 suBk9n']\"\n",
    "                        while EC.presence_of_element_located(By.XPATH, back_button_xpath) == True:\n",
    "                            actions = ActionChains(driver)\n",
    "                            actions.click(driver.find_element(By.XPATH, back_button_xpath))\n",
    "                            actions.perform()\n",
    "\n",
    "                            wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[@class='pDzPRp' and @type='password']\"))).send_keys('Noppawam1')\n",
    "                            login_confirm_xpath = \"//*[@class='wyhvVD _1EApiB hq6WM5 L-VL8Q cepDQ1 _7w24N1']\"\n",
    "                            wait.until(EC.element_to_be_clickable((By.XPATH, login_confirm_xpath)))\n",
    "                except:\n",
    "                    driver.quit()\n",
    "                    print(\"Error occured... retrying\")\n",
    "                    # time.sleep(30)\n",
    "                    driver = start_new_selenium_session(url)\n",
    "                    driver.execute_script(\"window.open('');\")\n",
    "                    driver.switch_to.window(driver.window_handles[1])\n",
    "                    driver.get(url_sub)\n",
    "                    wait = WebDriverWait(driver, 4)\n",
    "                \n",
    "        # scroll page\n",
    "        while True :\n",
    "            for i in range(1, total_height, 100):\n",
    "                driver.execute_script(\"window.scrollTo(0, {});\".format(i))\n",
    "            if EC.presence_of_element_located((By.XPATH, '//div[@class=\"_4Zf0hW PHkozQ\"]')) == True:\n",
    "                break\n",
    "            break\n",
    "\n",
    "        sel_source = Selector(text=driver.page_source)\n",
    "\n",
    "        #define pattern for regular expression\n",
    "        pattern = re.compile('(?:-i).(\\d*).(\\d*)')\n",
    "\n",
    "        # extract the data from the response using CSS or XPath selectors\n",
    "        shop_id = pattern.search(url_sub).group(1)\n",
    "        product_id = pattern.search(url_sub).group(2)\n",
    "        product_name = sel_source.xpath('//div[@class=\"YPqix5\"]/span/text()').get()\n",
    "        cat = sel_source.xpath('//a[@class=\"_4Zf0hW PHkozQ\"]/text()').getall()\n",
    "        cat_1 = cat[1]\n",
    "        try:\n",
    "            cat_2 = cat[2]\n",
    "        except:\n",
    "            cat_2 = \"\"\n",
    "        try:\n",
    "            cat_3 = cat[3]\n",
    "        except:\n",
    "            cat_3 = \"\"\n",
    "\n",
    "        price_detail_element = sel_source.xpath('//div[@class=\"flex items-center\"]/div/text()').getall()\n",
    "        price = price_detail_element[0]\n",
    "        if len(price_detail_element) != 1:\n",
    "            discount = price_detail_element[1]\n",
    "        star =  sel_source.xpath('//div[@class=\"yz-vZm _2qXJwX\"]/text()').get()\n",
    "        rating = sel_source.xpath('//div[@class=\"yz-vZm\"]/text()').get()\n",
    "        sold = sel_source.xpath('//div[@class=\"yiMptB\"]/text()').get()\n",
    "        color_available = sel_source.xpath('//div[@class=\"flex flex-column\"]/div[1]/div/button/text()').getall()\n",
    "        size_available = sel_source.xpath('//div[@class=\"flex flex-column\"]/div[2]/div/button/text()').getall()\n",
    "        stock = sel_source.xpath('//div[@class=\"flex flex-column\"]/div[last()]/div[2]/div[2]/text()').get().split()[0]\n",
    "        favorite = sel_source.xpath('//div[@class=\"_7na4jG\"]/text()').getall()[1].split()[1][1:-1]\n",
    "        # shop info\n",
    "        shop_info_url = f'https://XXXXX.co.th/api/v4/product/get_shop_info?shopid={shop_id}'\n",
    "        info = requests.get(shop_info_url)\n",
    "        info_dict = json.loads(info.text)\n",
    "\n",
    "        shop_verified = info_dict['data'][\"is_XXXXX_verified\"]\n",
    "        shop_preferred_plus_seller = info_dict['data'][\"is_preferred_plus_seller\"]\n",
    "        shop_location = info_dict['data'][\"shop_location\"]\n",
    "        shop_product = info_dict['data'][\"item_count\"]\n",
    "        shop_response_rate = info_dict['data'][\"response_rate\"]\n",
    "        shop_response_time = info_dict['data'][\"response_time\"]\n",
    "        shop_last_active = info_dict['data'][\"last_active_time\"]\n",
    "        shop_joined = sel_source.xpath('//div[@class=\"s1qcwz\"]/div[3]/div[1]/span/text()').get().split()\n",
    "        shop_followers = info_dict['data'][\"follower_count\"]\n",
    "        shop_star = info_dict['data'][\"rating_star\"]\n",
    "        shop_rating_bad = info_dict['data'][\"rating_bad\"]\n",
    "        shop_rating_good = info_dict['data'][\"rating_good\"]\n",
    "        shop_rating_normal = info_dict['data'][\"rating_normal\"]\n",
    "\n",
    "        ## create an instance of the item\n",
    "        # item = EcommerceScraperItem()\n",
    "        item = dict()\n",
    "\n",
    "        # assign the extracted data to the item field\n",
    "        for feature in features:\n",
    "            exec(f\"item['{feature}'] = {feature}\")\n",
    "        \n",
    "        ## check each extracted product \n",
    "        # print(item)\n",
    "        data.append(item)\n",
    "        print(f\"extracted product: {product_number}\")\n",
    "        product_number += 1\n",
    "\n",
    "        ## write json after each product extracted\n",
    "        # write_to_json([data], 'data.json')\n",
    "\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "driver.quit()\n",
    "\n",
    "print(\"scraping completed\")\n",
    "print(f\"{product_number} products are extracted\")\n",
    "\n",
    "# write json after all product extracted\n",
    "write_to_json([data], 'data.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9cff5a362bc38ef45d817ae74b1af54d6a076e3d773891282bce078b815ba34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
